{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def word_length(string):\n",
    "    '''This function returns the average word length in characters for the words in an item'''\n",
    "    #Get the length of the full text in characters\n",
    "    chars = len(string)\n",
    "    #Split the string into words\n",
    "    words = string.split()\n",
    "    #Compute the average word length and round the output to the second decimal point\n",
    "    if len(words)!=0:\n",
    "        avg_word_length = chars/len(words)\n",
    "   \n",
    "        return round(avg_word_length, 2)\n",
    "\n",
    "\n",
    "def sentence_counter(text):\n",
    "    '''This function returns the number of sentences in an item'''\n",
    "    doc = nlp(text)\n",
    "    #Initialize a counter variable\n",
    "    counter = 0\n",
    "    #Update the counter for each sentence which can be found in the doc.sents object returned by the Spacy model\n",
    "    for sentence in doc.sents:\n",
    "        counter = counter + 1\n",
    "    return counter\n",
    "\n",
    "#Note that this function is applied to the raw text in order to identify sentence boundaries\n",
    "\n",
    "def avg_sent_length(text):\n",
    "    '''This function returns the average sentence length in an item'''\n",
    "    doc = nlp(text)\n",
    "    #Initialize a counter variable\n",
    "    sent_number = 0\n",
    "    #Update the counter for each sentence which can be found in the doc.sents object returned by the Spacy model\n",
    "    for sent in doc.sents:\n",
    "        sent_number = sent_number + 1\n",
    "    #Get the number of words\n",
    "    words = text.split()\n",
    "    #Compute the average sentence length and round it to the second decimal point\n",
    "    avg_sent_length = len(words)/sent_number\n",
    "    return round(avg_sent_length, 2)\n",
    "\n",
    "#Note that this function is applied to the raw text in order to identify sentence boundaries\n",
    "\n",
    "\n",
    "def nouns(text, model=nlp):\n",
    "    '''This function returns the number of nouns in an item'''\n",
    "    # Create doc object \n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "\n",
    "\n",
    "def verbs(text, model=nlp):\n",
    "    '''This function returns the number of verbs in an item'''\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of verbs\n",
    "    return pos.count('VERB')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def adjectives(text, model=nlp):\n",
    "    '''This function returns the number of adjectives in an item'''\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adjectives\n",
    "    return pos.count('ADJ')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adverbs(text, model=nlp):\n",
    "    '''This function returns the number of adverbs in an item'''\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adverbs\n",
    "    return pos.count('ADV')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def numeral(text, model=nlp):\n",
    "    '''This function returns the number of numerals (e.g., billion) in an item'''\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adverbs\n",
    "    return pos.count('NUM')\n",
    "\n",
    "\n",
    "\n",
    "def aux(text, model=nlp):\n",
    "    '''This function returns the number of auxiliary in an item'''\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adverbs\n",
    "    return pos.count('AUX')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_nps(text):\n",
    "    '''This is a function that outputs the number of noun phrases in an item'''\n",
    "    doc = nlp(text)\n",
    "    NP_count = 0\n",
    "    for np in doc.noun_chunks:\n",
    "        NP_count = NP_count + 1\n",
    "    return NP_count\n",
    "    #print(np)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_pps(text):\n",
    "    '''This is a function that outputs the number of prepositional phrases in an item'''\n",
    "    doc = nlp(text)\n",
    "    pps = 0\n",
    "    for token in doc:\n",
    "        # You can try this with other parts of speech for different subtrees.\n",
    "        if token.pos_ == 'ADP':\n",
    "            \n",
    "            #Use the command below if you wanted to get the actual PPs\n",
    "            #pp = ' '.join([tok.orth_ for tok in token.subtree])\n",
    "            \n",
    "            #This command counts the number of PPs\n",
    "            pps = pps + 1\n",
    "            \n",
    "    return pps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pattern = [{'POS': 'VERB', 'OP': '?'},\n",
    "           {'POS': 'ADV', 'OP': '*'},\n",
    "           {'POS': 'AUX', 'OP': '*'},\n",
    "           {'POS': 'VERB', 'OP': '+'}]\n",
    "\n",
    "\n",
    "def get_vps(text):\n",
    "    '''This function returns the number of verb phrases in an item'''\n",
    "    doc = nlp(text)\n",
    "    vps = 0\n",
    "    # instantiate a Matcher instance\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Verb phrase\", [pattern], on_match=None) #new syntax of the command\n",
    "    # call the matcher to find matches \n",
    "    matches = matcher(doc)\n",
    "    spans = [doc[start:end] for _, start, end in matches]\n",
    "    for match in matches:\n",
    "        vps = vps +1\n",
    "    return vps\n",
    "\n",
    "\n",
    "temporal_connectives = ['afterwards', 'once', 'at this moment', 'at this point', 'before', 'finally', \n",
    "                        'here', 'in the end', 'lastly', 'later on', 'meanwhile', 'next', 'now', \n",
    "                        'on another occasion', 'previously','since', 'soon', 'straightaway', 'then', \n",
    "                        'when', 'whenever', 'while']\n",
    "\n",
    "\n",
    "#Connectives to show cause or conditions\n",
    "causal_connectives = ['accordingly', 'all the same', 'an effect of', 'an outcome of', 'an upshot of',\n",
    "                      'as a consequence of', 'as a result of', 'because', 'caused by', 'consequently',\n",
    "                      'despite this', 'even though', 'hence', 'however', 'in that case', 'moreover',\n",
    "                      'nevertheless', 'otherwise', 'so', 'so as', 'stemmed from', 'still', 'then',\n",
    "                      'therefore', 'though', 'under the circumstances', 'yet']\n",
    "\n",
    "\n",
    "#Connectives for showing results\n",
    "exemplifying_connectives = ['accordingly', 'as a result', 'as exemplified by', 'consequently', 'for example',\n",
    "                            'for instance', 'for one thing', 'including', 'provided that', 'since', 'so',\n",
    "                            'such as', 'then', 'therefore', 'these include', 'through', 'unless', 'without']\n",
    "\n",
    "\n",
    "#Connectives to show similarity or add a point\n",
    "additive_connectives = ['and', 'additionally', 'also', 'as well', 'even', 'furthermore', 'in addition', 'indeed',\n",
    "                        'let alone', 'moreover', 'not only']\n",
    "\n",
    "#Connectives showing a difference or an opposite point of view\n",
    "contrastive_connectives = ['alternatively', 'anyway', 'but', 'by contrast', 'differs from', 'elsewhere',\n",
    "                           'even so', 'however', 'in contrast', 'in fact', 'in other respects', 'in spite of this',\n",
    "                           'in that respect', 'instead', 'nevertheless', 'on the contrary', 'on the other hand',\n",
    "                           'rather', 'though', 'whereas', 'yet']\n",
    "\n",
    "\n",
    "def temporal_connectives_count(text):\n",
    "    '''This function counts the number of temporal connectives in a text'''\n",
    "    count = 0\n",
    "    for string in temporal_connectives:\n",
    "        for match in re.finditer(string, text):\n",
    "            count +=  1\n",
    "    return count\n",
    "\n",
    "\n",
    "def causal_connectives_count(text):\n",
    "    '''This function counts the number of causal connectives in a text'''\n",
    "    count = 0\n",
    "    for string in causal_connectives:\n",
    "        for match in re.finditer(string, text):\n",
    "            count +=  1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "def exemplifying_connectives_count(text):\n",
    "    '''This function counts the number of exemplifying connectives in a text'''\n",
    "    count = 0\n",
    "    for string in exemplifying_connectives:\n",
    "        for match in re.finditer(string, text):\n",
    "            count +=  1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "def additive_connectives_count(text):\n",
    "    '''This function counts the number of additive connectives in a text'''\n",
    "    count = 0\n",
    "    for string in additive_connectives:\n",
    "        for match in re.finditer(string, text):\n",
    "            count +=  1\n",
    "    return count\n",
    "\n",
    "\n",
    "def contrastive_connectives_count(text):\n",
    "    '''This function counts the number of contrastive connectives in a text'''\n",
    "    cont_con = 0\n",
    "    for string in contrastive_connectives:\n",
    "        if string in text:\n",
    "            cont_con = cont_con + 1\n",
    "    return cont_con\n",
    "\n",
    "\n",
    "def filled_pauses(text):\n",
    "\n",
    "    filled_pause = [\"uhm\"]\n",
    "\n",
    "    cont_pauses = 0\n",
    "    for string in filled_pause:\n",
    "        for match in re.finditer(string, text):\n",
    "            cont_pauses += 1\n",
    "    return cont_pauses\n",
    "\n",
    "df['Filled_Pauses'] = df['sentence'].apply(filled_pauses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "\n",
     "from lexicalrichness import LexicalRichness\n",
     "import os\n",
     "import pandas as pd\n",
     "import csv\n",
     "import glob\n",
     "import numpy as np\n",
     "import os\n",
     "import sys\n",
     "import sox\n",
     "import shutil\n",
     "from nltk.corpus import stopwords\n",
     "from operator import itemgetter\n",
     "from itertools import groupby\n",
     "import numpy as np\n",
     "import shutil \n",
     "import pandas as pd\n",
     "from scipy import stats\n",
     "import glob\n",
     "import matplotlib.pyplot as plt\n",
     "from scipy.stats import ranksums\n",
     "import seaborn as sns\n",
     "import re\n",
     "import spacy\n",
     "import en_core_web_sm\n",
     "import glob\n",
     "from spacy.matcher import Matcher\n",
     "import csv\n",
     "import os\n",
     "import sys\n",
     "\n",
     "import shutil\n",
     "from operator import itemgetter\n",
     "from itertools import groupby\n",
     "import numpy as np\n",
     "import shutil \n",
     "import pandas as pd\n",
     "from scipy import stats\n",
     "import glob\n",
     "import matplotlib.pyplot as plt\n",
     "from scipy.stats import ranksums\n",
     "import pandas as pd\n",
     "import numpy as np\n",
     "from nltk.tokenize import word_tokenize\n",
     "from nltk.tokenize import sent_tokenize\n",
     "from nltk.corpus import stopwords\n",
     "from collections import Counter\n",
     "import re\n",
     "import spacy\n",
     "from scipy.stats import ranksums\n",
     "import en_core_web_sm\n",
     "import glob\n",
     "from spacy.matcher import Matcher\n",
     "\n",
     "\n",
     "\n",
     "def compute_lexical_diversity(transcript):\n",
     "    \n",
     "    lex = LexicalRichness(transcript)\n",
     "   # word_count = lex.words\n",
     "    unique_word_count =  lex.terms\n",
     "    type_token_ratio = lex.ttr\n",
     "   # root_type_token_ratio = lex.rttr\n",
     "    corrected_type_token_ratio = lex.cttr\n",
     "   # mean_segmental_type_token_ratio = lex.msttr(segment_window=12) #25\n",
     "    moving_average_type_token_ratio = lex.mattr(window_size=13) #25\n",
     "   # measure_textual_lexical_diversity= lex.mtld(threshold=0.72)\n",
     "   # hypergeometric_distribution_diversity = lex.hdd(draws=13)\n",
     "   # herdan_lexical_diversity_measure = lex.Herdan\n",
     "    summer_lexical_diversity_measure=lex.Summer\n",
     "    dugast_lexical_diversity_measure =lex.Dugast\n",
     "   # maas_lexical_diversity_measure = lex.Maas\n",
     "    \n",
     "    return unique_word_count, type_token_ratio, corrected_type_token_ratio, moving_average_type_token_ratio, summer_lexical_diversity_measure, dugast_lexical_diversity_measure\n",
     "    \n",
     "\n",
     "\n",
     "def load_files(data):\n",
     "    \n",
     "    speakers = data['idx'].tolist()\n",
     "    sentences = data['sentence'].tolist()\n",
     "    labels = data['label'].tolist()\n",
     "    lex_vals = np.array([compute_lexical_diversity(sent) for sent in sentences])\n",
     "    names = [\"unique_word_count\", \"type_token_ratio\", \"corrected_type_token_ratio\", \"moving_average_type_token_ratio\", \"summer_lexical_diversity_measure\", \"dugast_lexical_diversity_measure\"]\n",
     "    frame = pd.DataFrame({\"speakers\": speakers, \"labels\": labels, \"sentences\": sentences, **{name:val for name, val in zip(names,lex_vals.T)}})\n",
     "    return frame\n",
     "    \n",
     "\n",
     "\n",
     "\n",
     "\n",
     "### LINGUISTIC FEATURES\n",
     "\n",
     "\n",
     "nlp = spacy.load('en_core_web_sm')\n",
     "\n",
     "# Create a function to preprocess the text\n",
     "\n",
     "\n",
     "#Customized list of stopwords\n",
     "stopwords = list(stopwords.words('english'))\n",
     "\n",
     "def preprocess(text):\n",
     "    '''This is a function to perform tokenization, lemmatization, removal of non-alphabetic characters\n",
     "    and stopword removal'''\n",
     "  # Create Doc object\n",
     "    doc = nlp(text, disable=['ner'])\n",
     "    # Generate lemmas\n",
     "    lemmas = [token.lemma_ for token in doc]\n",
     "    # Remove stopwords and non-alphabetic characters\n",
     "    a_lemmas = [lemma for lemma in lemmas \n",
     "            if lemma.isalpha() and lemma not in stopwords]\n",
     "    return ' '.join(a_lemmas)\n",
     "\n",
     "\n",
     "\n",
     "def count_words(string):\n",
     "    '''This function returns the number of words in a string'''\n",
     "    # Split the string into words\n",
     "    words = string.split()\n",
     "    # Return the number of words\n",
     "    return len(words)\n",
     "\n",
     "#Application to the raw data to get the full word count\n",
     "\n",
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}