{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Linguistic Feature Extraction with Spacy using ADDRESSO 2021 data set\n",
    "\n",
    "- Lexical features\n",
    "- Syntactic features\n",
    "- Features of cohesion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = '/export/b15/rpapagari/Tianzi_work/ADReSSo_NoVAD_IS2021_dataset/data_ADReSSo_diagnosis_cv10_text_v7_Longformer_TrainDevTest/cv_6/utt2csvpath'\n",
    "train = '/export/b15/rpapagari/Tianzi_work/ADReSSo_NoVAD_IS2021_dataset/data_ADReSSo_diagnosis_cv10_text_v7_Longformer_TrainDevTest/cv_6/train.tsv'\n",
    "dev = '/export/b15/rpapagari/Tianzi_work/ADReSSo_NoVAD_IS2021_dataset/data_ADReSSo_diagnosis_cv10_text_v7_Longformer_TrainDevTest/cv_6/dev.tsv'\n",
    "test = '/export/b15/rpapagari/Tianzi_work/ADReSSo_NoVAD_IS2021_dataset/data_ADReSSo_diagnosis_cv10_text_v7_Longformer_TrainDevTest/cv_6/test.tsv'\n",
    "\n",
    "\n",
    "def data_to_csv(path_trans, train, dev, test):\n",
    "    \n",
    "    path_ordered = []\n",
    "    sentences = []\n",
    "    \n",
    "    read_train = pd.read_csv(train, header=None)\n",
    "    read_dev = pd.read_csv(dev, header=None)\n",
    "    read_test = pd.read_csv(test, header=None)\n",
    "    data = pd.concat([read_train, read_dev, read_test], ignore_index=True)\n",
    "    \n",
    "    read_trans = pd.read_csv(path_trans, header=None)\n",
    "    patients = (data[0].tolist())\n",
    "    labels = (data[1].tolist())\n",
    "    path_to_transcript= read_trans[1].tolist()\n",
    "    \n",
    "    for patient in patients:\n",
    "        for path in path_to_transcript:\n",
    "            if os.path.basename(path).split('.csv')[0] == patient:\n",
    "                path_ordered.append(path)\n",
    "                \n",
    "    for transcript in path_ordered:\n",
    "        with open(transcript, 'r') as f:\n",
    "            transcript_ = f.readlines()\n",
    "        #print(transcript_)\n",
    "            transcript_ = transcript_[0]\n",
    "            sentences.append(transcript_)\n",
    "\n",
    "            \n",
    "    \n",
    "    dict = {'idx': patients, 'label': labels, 'sentence': sentences} \n",
    "    df = pd.DataFrame(dict)\n",
    "    return df\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "df = data_to_csv(path, train, dev, test)\n",
    "\n",
    "\n",
    "## Pre-Processing "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We perform the following preprocessing steps:\n",
    "\n",
    "- **Convert all words to lowercase**. This is important for matching certain phrases later on, where the matching is case-sensitive and the word or phrase of interest would not be matched to a capitalized version of the same word or phrase.\n",
    "\n",
    "- **Tokenize the data**. This step refers to identifying the boundaries of individual words and sentences within each item.\n",
    "\n",
    "- **Lemmatization**. This process converts each word to its base form (e.g. 'has' to 'have', 'are' to 'be') for the purpose of easier matching.\n",
    "\n",
    "- **Removal of words that contain non-alphabetic characters** (e.g., numbers, '#', '@', etc.)\n",
    "\n",
    "- **Stopword removal**. This step refers to the removal of words that may not be important for the analysis (e.g. 'a', 'an', 'the', etc.). One could use a predefined list of stopwords (usually these remove all forms of the verb 'to be', as well as pronouns) or create a customized list of stopwords depending on what is important for the specific application.\n",
    "\n",
    "\n",
    "\n",
    "def compute_lexical_diversity(transcript):\n",
    "    \n",
    "    lex = LexicalRichness(transcript)\n",
    "   # word_count = lex.words\n",
    "    unique_word_count =  lex.terms\n",
    "    type_token_ratio = lex.ttr\n",
    "   # root_type_token_ratio = lex.rttr\n",
    "    corrected_type_token_ratio = lex.cttr\n",
    "   # mean_segmental_type_token_ratio = lex.msttr(segment_window=12) #25\n",
    "    moving_average_type_token_ratio = lex.mattr(window_size=13) #25\n",
    "   # measure_textual_lexical_diversity= lex.mtld(threshold=0.72)\n",
    "   # hypergeometric_distribution_diversity = lex.hdd(draws=13)\n",
    "   # herdan_lexical_diversity_measure = lex.Herdan\n",
    "    summer_lexical_diversity_measure=lex.Summer\n",
    "    dugast_lexical_diversity_measure =lex.Dugast\n",
    "   # maas_lexical_diversity_measure = lex.Maas\n",
    "    \n",
    "    return unique_word_count, type_token_ratio, corrected_type_token_ratio, moving_average_type_token_ratio, summer_lexical_diversity_measure, dugast_lexical_diversity_measure\n",
    "    \n",
    "\n",
    "\n",
    "def load_files(data):\n",
    "    \n",
    "    speakers = data['idx'].tolist()\n",
    "    sentences = data['sentence'].tolist()\n",
    "    labels = data['label'].tolist()\n",
    "    lex_vals = np.array([compute_lexical_diversity(sent) for sent in sentences])\n",
    "    names = [\"unique_word_count\", \"type_token_ratio\", \"corrected_type_token_ratio\", \"moving_average_type_token_ratio\", \"summer_lexical_diversity_measure\", \"dugast_lexical_diversity_measure\"]\n",
    "    frame = pd.DataFrame({\"speakers\": speakers, \"labels\": labels, \"sentences\": sentences, **{name:val for name, val in zip(names,lex_vals.T)}})\n",
    "    return frame\n",
    "\n",
    "\n",
    "df = load_files(df)\n",
    "\n",
    "\n",
    "#load the Spacy model for extracting data for English: \"en_core_web_sm\" \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "#lower case the transcript \n",
    "df['sentence'] = df['sentence'].str.lower()\n",
    "\n",
    "\n",
    "# Create a function to preprocess the text\n",
    "\n",
    "#Customized list of stopwords \n",
    "stopwords = ['a', 'an', 'the', 'with', 'to', 'be', 'have', 'for', 'has']\n",
    "\n",
    "def preprocess(text):\n",
    "    '''This is a function to perform tokenization, lemmatization, removal of non-alphabetic characters\n",
    "    and stopword removal'''\n",
    "  # Create Doc object\n",
    "    doc = nlp(text, disable=['ner'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    return ' '.join(a_lemmas)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "df['Item'] = df['sentence'].apply(preprocess)\n",
    "\n",
    "\n",
    "# Shallow Features\n",
    "\n",
    "## Count Words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def count_words(string):\n",
    "    '''This function returns the number of words in a string'''\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "#Application to the raw data to get the full word count\n",
    "\n",
    "df['Word_Count'] = df['sentence'].apply(count_words)\n",
    "\n",
    "#Application to the preprocessed data to get the content-word count\n",
    "\n",
    "df['Word_Count_No_stop_words'] = df['Item'].apply(count_words)\n",
    "df.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word Length"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def word_length(string):\n",
    "    '''This function returns the average word length in characters for the words in an item'''\n",
    "    #Get the length of the full text in characters\n",
    "    chars = len(string)\n",
    "    #Split the string into words\n",
    "    words = string.split()\n",
    "    #Compute the average word length and round the output to the second decimal point\n",
    "    avg_word_length = chars/len(words)\n",
    "    return round(avg_word_length, 2)\n",
    "\n",
    "#Application to the preprocessed data\n",
    "\n",
    "df['Avg_Word_Length'] = df['Item'].apply(word_length)\n",
    "#df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sentence Counter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sentence_counter(text):\n",
    "    '''This function returns the number of sentences in an item'''\n",
    "    doc = nlp(text)\n",
    "    #Initialize a counter variable\n",
    "    counter = 0\n",
    "    #Update the counter for each sentence which can be found in the doc.sents object returned by the Spacy model\n",
    "    for sentence in doc.sents:\n",
    "        counter = counter + 1\n",
    "    return counter\n",
    "\n",
    "#Note that this function is applied to the raw text in order to identify sentence boundaries\n",
    "\n",
    "df['Sentence_Count'] = df['sentence'].apply(sentence_counter)\n",
    "\n",
    "\n",
    "### Average Sentence Lenght in Words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def avg_sent_length(text):\n",
    "    '''This function returns the average sentence length in an item'''\n",
    "    doc = nlp(text)\n",
    "    #Initialize a counter variable\n",
    "    sent_number = 0\n",
    "    #Update the counter for each sentence which can be found in the doc.sents object returned by the Spacy model\n",
    "    for sent in doc.sents:\n",
    "        sent_number = sent_number + 1\n",
    "    #Get the number of words\n",
    "    words = text.split()\n",
    "    #Compute the average sentence length and round it to the second decimal point\n",
    "    avg_sent_length = len(words)/sent_number\n",
    "    return round(avg_sent_length, 2)\n",
    "\n",
    "#Note that this function is applied to the raw text in order to identify sentence boundaries\n",
    "df['Avg_Sentence_Length_in_Words'] = df['sentence'].apply(avg_sent_length)\n",
    "#df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Syntactic Features\n",
    "\n",
    "https://spacy.io/usage/linguistic-features#pos-tagging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Noun Count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def nouns(text, model=nlp):\n",
    "    '''This function returns the number of nouns in an item'''\n",
    "    # Create doc object \n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "df['Noun_Count'] = df['Item'].apply(nouns)\n",
    "#df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Verb Count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def verbs(text, model=nlp):\n",
    "    '''This function returns the number of verbs in an item'''\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of verbs\n",
    "    return pos.count('VERB')\n",
    "\n",
    "df['Verb_Count'] = df['Item'].apply(verbs)\n",
    "\n",
    "#df.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adjective Count "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adjectives(text, model=nlp):\n",
    "    '''This function returns the number of adjectives in an item'''\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adjectives\n",
    "    return pos.count('ADJ')\n",
    "\n",
    "df['Adjective_Count'] = df['Item'].apply(adjectives)\n",
    "\n",
    "#df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adverb Count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adverbs(text, model=nlp):\n",
    "    '''This function returns the number of adverbs in an item'''\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adverbs\n",
    "    return pos.count('ADV')\n",
    "\n",
    "df['Adverb_Count'] = df['Item'].apply(adverbs)\n",
    "#df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Numeral Count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def numeral(text, model=nlp):\n",
    "    '''This function returns the number of numerals (e.g., billion) in an item'''\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adverbs\n",
    "    return pos.count('NUM')\n",
    "\n",
    "df['Numeral Count'] = df['sentence'].apply(numeral) #meglio estrarlo dall'originale\n",
    "#df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Auxiliary Count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def aux(text, model=nlp):\n",
    "    '''This function returns the number of auxiliary in an item'''\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adverbs\n",
    "    return pos.count('AUX')\n",
    "\n",
    "df['Auxiliary_Count'] = df['sentence'].apply(aux) #meglio estrarlo dall'originale\n",
    "#df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Number Noun Phrases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_nps(text):\n",
    "    '''This is a function that outputs the number of noun phrases in an item'''\n",
    "    doc = nlp(text)\n",
    "    NP_count = 0\n",
    "    for np in doc.noun_chunks:\n",
    "        NP_count = NP_count + 1\n",
    "    return NP_count\n",
    "    #print(np)\n",
    "\n",
    "df['Number_of_NPs'] = df['Item'].apply(get_nps)\n",
    "#df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Number Prepositioanal Phrases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_pps(text):\n",
    "    '''This is a function that outputs the number of prepositional phrases in an item'''\n",
    "    doc = nlp(text)\n",
    "    pps = 0\n",
    "    for token in doc:\n",
    "        # You can try this with other parts of speech for different subtrees.\n",
    "        if token.pos_ == 'ADP':\n",
    "            \n",
    "            #Use the command below if you wanted to get the actual PPs\n",
    "            #pp = ' '.join([tok.orth_ for tok in token.subtree])\n",
    "            \n",
    "            #This command counts the number of PPs\n",
    "            pps = pps + 1\n",
    "            \n",
    "    return pps\n",
    "\n",
    "df['Number_of_PPs'] = df['Item'].apply(get_pps)\n",
    "#df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Number Verb Phrases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pattern = [{'POS': 'VERB', 'OP': '?'},\n",
    "           {'POS': 'ADV', 'OP': '*'},\n",
    "           {'POS': 'AUX', 'OP': '*'},\n",
    "           {'POS': 'VERB', 'OP': '+'}]\n",
    "\n",
    "\n",
    "def get_vps(text):\n",
    "    '''This function returns the number of verb phrases in an item'''\n",
    "    doc = nlp(text)\n",
    "    vps = 0\n",
    "    # instantiate a Matcher instance\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Verb phrase\", [pattern], on_match=None) #new syntax of the command\n",
    "    # call the matcher to find matches \n",
    "    matches = matcher(doc)\n",
    "    spans = [doc[start:end] for _, start, end in matches]\n",
    "    for match in matches:\n",
    "        vps = vps +1\n",
    "    return vps\n",
    "    \n",
    "df['Number_of_VPs'] = df['Item'].apply(get_vps)\n",
    "#df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Features of Cohesion\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#First, we create lists of different types of connectives that we will later match to the text\n",
    "\n",
    "#Connectives to instruct, recount and sequence\n",
    "temporal_connectives = ['afterwards', 'once', 'at this moment', 'at this point', 'before', 'finally', \n",
    "                        'here', 'in the end', 'lastly', 'later on', 'meanwhile', 'next', 'now', \n",
    "                        'on another occasion', 'previously','since', 'soon', 'straightaway', 'then', \n",
    "                        'when', 'whenever', 'while']\n",
    "\n",
    "\n",
    "#Connectives to show cause or conditions\n",
    "causal_connectives = ['accordingly', 'all the same', 'an effect of', 'an outcome of', 'an upshot of',\n",
    "                      'as a consequence of', 'as a result of', 'because', 'caused by', 'consequently',\n",
    "                      'despite this', 'even though', 'hence', 'however', 'in that case', 'moreover',\n",
    "                      'nevertheless', 'otherwise', 'so', 'so as', 'stemmed from', 'still', 'then',\n",
    "                      'therefore', 'though', 'under the circumstances', 'yet']\n",
    "\n",
    "\n",
    "#Connectives for showing results\n",
    "exemplifying_connectives = ['accordingly', 'as a result', 'as exemplified by', 'consequently', 'for example',\n",
    "                            'for instance', 'for one thing', 'including', 'provided that', 'since', 'so',\n",
    "                            'such as', 'then', 'therefore', 'these include', 'through', 'unless', 'without']\n",
    "\n",
    "\n",
    "#Connectives to show similarity or add a point\n",
    "additive_connectives = ['and', 'additionally', 'also', 'as well', 'even', 'furthermore', 'in addition', 'indeed',\n",
    "                        'let alone', 'moreover', 'not only']\n",
    "\n",
    "#Connectives showing a difference or an opposite point of view\n",
    "contrastive_connectives = ['alternatively', 'anyway', 'but', 'by contrast', 'differs from', 'elsewhere',\n",
    "                           'even so', 'however', 'in contrast', 'in fact', 'in other respects', 'in spite of this',\n",
    "                           'in that respect', 'instead', 'nevertheless', 'on the contrary', 'on the other hand',\n",
    "                           'rather', 'though', 'whereas', 'yet']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Temporal Connectives Count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def temporal_connectives_count(text):\n",
    "    '''This function counts the number of temporal connectives in a text'''\n",
    "    count = 0\n",
    "    for string in temporal_connectives:\n",
    "        for match in re.finditer(string, text):\n",
    "            count +=  1\n",
    "    return count\n",
    "\n",
    "#Note that we apply the function to the raw text (and remember that it is important to lowercase all words)\n",
    "df['Temporal_Connectives_Count'] = df['sentence'].apply(temporal_connectives_count)\n",
    "\n",
    "\n",
    "\n",
    "### Causal Connectives Count\n",
    "\n",
    "\n",
    "def causal_connectives_count(text):\n",
    "    '''This function counts the number of causal connectives in a text'''\n",
    "    count = 0\n",
    "    for string in causal_connectives:\n",
    "        for match in re.finditer(string, text):\n",
    "            count +=  1\n",
    "    return count\n",
    "\n",
    "df['Causal_Connectives_Count'] = df['sentence'].apply(causal_connectives_count)\n",
    "#df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exemplifying Connectives Count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def exemplifying_connectives_count(text):\n",
    "    '''This function counts the number of exemplifying connectives in a text'''\n",
    "    count = 0\n",
    "    for string in exemplifying_connectives:\n",
    "        for match in re.finditer(string, text):\n",
    "            count +=  1\n",
    "    return count\n",
    "\n",
    "df['Exemplifying_?Connectives_Count'] = df['sentence'].apply(exemplifying_connectives_count)\n",
    "#df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Additive Connectives Count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def additive_connectives_count(text):\n",
    "    '''This function counts the number of additive connectives in a text'''\n",
    "    count = 0\n",
    "    for string in additive_connectives:\n",
    "        for match in re.finditer(string, text):\n",
    "            count +=  1\n",
    "    return count\n",
    "\n",
    "df['Additive_Connectives_Count'] = df['sentence'].apply(additive_connectives_count)\n",
    "#df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Contrastive connectives Count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def contrastive_connectives_count(text):\n",
    "    '''This function counts the number of contrastive connectives in a text'''\n",
    "    cont_con = 0\n",
    "    for string in contrastive_connectives:\n",
    "        if string in text:\n",
    "            cont_con = cont_con + 1\n",
    "    return cont_con\n",
    "\n",
    "df['Contrastive_Connectives_Count'] = df['sentence'].apply(contrastive_connectives_count)\n",
    "#df.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filled_pause = [\"uhm\"]\n",
    "    \n",
    "\n",
    "def filled_pauses(text):\n",
    "    \n",
    "    cont_pauses = 0\n",
    "    for string in filled_pause:\n",
    "        for match in re.finditer(string, text):\n",
    "            cont_pauses += 1\n",
    "    return cont_pauses\n",
    "\n",
    "df['Filled_Pauses'] = df['sentence'].apply(filled_pauses)\n",
    "#df[:40]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def certanty(text):\n",
    "    \n",
    "    '''Function design to capture the level of certainty of patients in providing the description\n",
    "    of the image. To operationalize uncertanty I chose modals verbs as cue and interrogative marks. '''\n",
    "    \n",
    "    cont_con = 0\n",
    "    if \"?\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"why\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"might\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"can\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"may\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"sure\" in text:\n",
    "        cont_con = cont_con + 1     \n",
    "   # if \"I\" in text:\n",
    "      #  cont_con = cont_con + 1 \n",
    "    if \"uhm\" in text:\n",
    "        cont_con = cont_con + 1 \n",
    "    if \"ah\" in text:\n",
    "        cont_con = cont_con + 1 \n",
    "    if \"should\" in text:\n",
    "        cont_con = cont_con + 1 \n",
    "    if \"looks like\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "        \n",
    "    \n",
    "    return cont_con\n",
    "\n",
    "df['certanty'] = df['sentence'].apply(certanty)\n",
    "\n",
    "#%\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}