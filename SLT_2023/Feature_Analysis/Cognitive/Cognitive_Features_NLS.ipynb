{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bbe72aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "def compute_auc(array_1, array_2):\n",
    "    '''Compute AUROC.'''\n",
    "    xs = np.concatenate([array_1, array_2],axis=1)\n",
    "    y = np.concatenate([array_1.shape[1]*[2], array_2.shape[1]*[1]])\n",
    "\n",
    "    for i, x in enumerate(xs):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, x, pos_label=2)\n",
    "        #print(i, metrics.auc(fpr, tpr))\n",
    "        m = metrics.roc_auc_score(y, x)\n",
    "        print(round(max(m, 1-m), 2))\n",
    "\n",
    "\n",
    "\n",
    "def delete_multiple_element(list_object, indices):\n",
    "    indices = sorted(indices, reverse=True)\n",
    "    for idx in indices:\n",
    "        if idx < len(list_object):\n",
    "            list_object.pop(idx)\n",
    "            \n",
    "    return list_object\n",
    "\n",
    "\n",
    "\n",
    "def kruskal(f, task, c, p, c_name, p_name):\n",
    "    for i, title in enumerate(task):\n",
    "        nome = title\n",
    "        f.write(('\\n'+ f'kruskal results for {title} {c_name} {p_name} {stats.kruskal(c[i], p[i])} \\n\\n'))\n",
    "\n",
    "def holm_correction(kruskal):\n",
    "    line_to_remove=[]\n",
    "    values=[]\n",
    "    corrected =[]\n",
    "    final = []\n",
    "    for l in kruskal:\n",
    "        if \"nan\" in l:\n",
    "            line_to_remove.append(kruskal.index(l))\n",
    "    \n",
    "    new_krusk = delete_multiple_element(kruskal, line_to_remove)\n",
    "            \n",
    "    for line in new_krusk:\n",
    "        ok = line.split('vs.')[1]\n",
    "        num = ok.split(\" \")[2]\n",
    "        values.append(float(num))\n",
    "   # values = [x for x in values if isnan(x) == False]\n",
    "    result = statsmodels.stats.multitest.fdrcorrection(values, alpha=0.05, method='indep', is_sorted=False)\n",
    "    num = np.where(result[0] == True)\n",
    "    list_index = ((num)[0]).tolist()\n",
    "\n",
    "    for i in list_index:\n",
    "        corrected.append(result[1][i])\n",
    "    for i in list_index:\n",
    "        final.append(kruskal[i])\n",
    "    \n",
    "    return final, corrected\n",
    "    \n",
    "#Speech transcripts Cookie Theft picture description task\n",
    "\n",
    "input_dir = '/export/c12/afavaro/Transcriptions/CTP/'\n",
    "\n",
    "\n",
    "path = []\n",
    "for ok in os.listdir(input_dir):\n",
    "    if ok.endswith('.txt'):\n",
    "        path.append(os.path.join(input_dir, ok))\n",
    "len(path)   \n",
    "\n",
    "transcr = []\n",
    "for file in path:\n",
    "    with open(file, \"r\") as f:\n",
    "        string_without_line_breaks = \"\"\n",
    "        for line in f:\n",
    "            stripped_line = line.rstrip()\n",
    "            string_without_line_breaks += stripped_line\n",
    "        transcr.append(string_without_line_breaks)\n",
    "\n",
    "speaker = [os.path.basename(file).split(\"_ses\")[0] for file in path]\n",
    "dataframe = {'idx':speaker, 'sentence': transcr}\n",
    "df_10msec = pd.DataFrame(dataframe)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Add labels\n",
    "\n",
    "label_seneca = pd.read_excel(\"/export/b14/afavaro/Acoustic_Features/DigiPsych_Prosody/Hospital_Analysis/Book3.xlsx\")\n",
    "label = label_seneca['Label'].tolist()\n",
    "speak = label_seneca['Participant I.D.'].tolist() #id\n",
    "spk2lab_ = {sp:lab for sp,lab in zip(speak,label)}\n",
    "speak2__ = df_10msec['idx'].tolist()\n",
    "\n",
    "\n",
    "etichettex = []\n",
    "for nome in speak2__:\n",
    "    if nome in spk2lab_.keys():\n",
    "        lav = spk2lab_[nome]\n",
    "        etichettex.append(([nome, lav]))\n",
    "    else:\n",
    "         etichettex.append(([nome, 'Unknown']))\n",
    "\n",
    "\n",
    "\n",
    "label_new_ = []\n",
    "for e in etichettex:\n",
    "    label_new_.append(e[1])\n",
    "df_10msec['label'] = label_new_\n",
    "\n",
    "\n",
    "\n",
    "def uncertanty(text):\n",
    "    \n",
    "    ''' Function design to capture the level of certainty of patients of participants when delivering  the description\n",
    "    of the image.  '''\n",
    "    \n",
    "    cont_con = 0\n",
    "    if \"?\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"why\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"might\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    #if \"can\" in text:\n",
    "      #  cont_con = cont_con + 1\n",
    "    if \"could\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"may\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    #if \"not sure\" in text:\n",
    "       # cont_con = cont_con + 1     \n",
    "   # if \"I\" in text:\n",
    "      #  cont_con = cont_con + 1 \n",
    "    if \"um\" in text:\n",
    "        cont_con = cont_con + 1 \n",
    "    if \"uhm\" in text:\n",
    "        cont_con = cont_con + 1 \n",
    "    if \"ah\" in text:\n",
    "        cont_con = cont_con + 1 \n",
    "    if \"perhaps\" in text:\n",
    "        cont_con = cont_con + 1 \n",
    "    #if \"should\" in text:\n",
    "        #cont_con = cont_con + 1 \n",
    "    if \"looks like\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "        \n",
    "    \n",
    "    return cont_con\n",
    "\n",
    "df['uncertanty'] = df['sentence'].apply(uncertanty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e39e93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def repetitions(text):\n",
    "     \n",
    "    '''Function design to capture the redundancy in the code.  To operationalize \n",
    "    redundancy I chose to count the repetitions. '''\n",
    "    \n",
    "    \n",
    "    repetition = 0\n",
    "    \n",
    "    text = text.split()\n",
    "    d = dict()\n",
    "    \n",
    "    for line in text:\n",
    "        line = line.strip()\n",
    "        line = line.lower()\n",
    "        words = line.split(\" \")\n",
    "        for word in words:\n",
    "\n",
    "            if word in d:\n",
    "                d[word] = d[word] + 1\n",
    "            else:\n",
    "                d[word] = 1\n",
    "    \n",
    "    for key in list(d.keys()):\n",
    "        if key != 'the' and key != \"a\" and key != \"of\" and key != \"to\" and key !=\"on\" and key !=\"is\" and key !=\"are\" and key !=\"in\" and key != \"an\":\n",
    "            if d[key] > 1:\n",
    "                repetition +=1\n",
    "\n",
    "    return repetition\n",
    "\n",
    "\n",
    "df['repetition'] = df['sentence'].apply(repetitions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ece98427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def informational_verb(text):\n",
    "    \n",
    "    '''Exhaustivness of the description operationalized by \n",
    "    counting how many (if any) salient items (nouns) are mentioned. '''\n",
    "    \n",
    "    cont_con = 0\n",
    "    \n",
    "    if \"washing\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"wash\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    \n",
    "    if \"overflowing\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "      \n",
    "    if \"overflow\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "        \n",
    "    if \"hanging\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"hang\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "        \n",
    "    if \"falling\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"fall\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "        \n",
    "    if \"wearing\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"wear\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"running\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "        \n",
    "    if \"run\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "        \n",
    "    if \"drying\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"dry\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"paying attention\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"reaching\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"reach\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"tipping\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"tipp\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "\n",
    "\n",
    "    \n",
    "    return cont_con\n",
    "\n",
    "df['informational_verb'] = df['sentence'].apply(informational_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "991a7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def informational_content(text):\n",
    "\n",
    "    '''Exhaustivness of the description operationalized by \n",
    "    counting how many (if any) salient actions (verbs) are mentioned. '''\n",
    "\n",
    "    \n",
    "    cont_con = 0\n",
    "    \n",
    "    if \"mother\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"sister\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"cookie\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"cookie jar\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"curtains\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"cabinet\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"brother\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"chair\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"kitchen\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"sink\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"garden\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"fall\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"dishes\" in text:\n",
    "        cont_con = cont_con + 1     \n",
    "    if \"stool\" in text:\n",
    "        cont_con = cont_con + 1 \n",
    "    if \"poddle\" in text:\n",
    "        cont_con = cont_con + 1 \n",
    "    if \"shoes\" in text:\n",
    "        cont_con = cont_con + 1 \n",
    "    if \"apron\" in text:\n",
    "        cont_con = cont_con + 1 \n",
    "    \n",
    "    return cont_con\n",
    "\n",
    "df['informational'] = df['sentence'].apply(informational_content)\n",
    "\n",
    "\n",
    "def ratio_info_rep_plus_uncert(df_): \n",
    "    \n",
    "    '''Ratio between repetitions and uncertanty,\n",
    "    where uncertainty is operationalized as repetition + uncertanty. '''\n",
    "    \n",
    "    summation = df_['repetition'] + df_[\"uncertanty\"]\n",
    "    ratio = df_['informational'] / summation #info / rep + uncertanty\n",
    "    df_[\"ratio_info_rep_plus_uncert\"] = ratio\n",
    "    \n",
    "    return df_\n",
    "\n",
    "df = ratio_info_rep_plus_uncert(df)\n",
    "\n",
    "\n",
    "def ratio_rep_certanty(df_): \n",
    "    \n",
    "    '''Function designed to measure the ratio between repetitions and uncertanty,\n",
    "    where uncertainty is intended as uncertainty. '''\n",
    "    \n",
    "    division = df_['repetition'] / df_[\"uncertanty\"] # repetition / uncertainty\n",
    "    df_[\"ratio_rep_certanty\"] = division\n",
    "    \n",
    "    return df_\n",
    "\n",
    "df = ratio_rep_certanty(df)\n",
    "\n",
    "\n",
    "\n",
    "grouped = df.groupby('label')\n",
    "\n",
    "\n",
    "control = grouped.get_group(\"CTRL\") \n",
    "parkinson_ = grouped.get_group(\"PD\")\n",
    "alzheimer =  grouped.get_group(\"AD\")\n",
    "#ataxia =  grouped.get_group(\"ATX\")\n",
    "others =  pd.concat([ grouped.get_group('CBS'),  \n",
    "                     grouped.get_group('ET'), grouped.get_group('GSS'), grouped.get_group('WD'), \n",
    "                      grouped.get_group('PSP'), grouped.get_group('MIM')])\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "# Remve probable PD subjects\n",
    "\n",
    "parkinson_ = parkinson_[~parkinson_.idx.str.contains(\"NLS_102\")]\n",
    "parkinson_ = parkinson_[~parkinson_.idx.str.contains(\"NLS_98\")]\n",
    "parkinson_ = parkinson_[~parkinson_.idx.str.contains(\"NLS_95\")]\n",
    "parkinson_ = parkinson_[~parkinson_.idx.str.contains(\"NLS_87\")]\n",
    "parkinson_ = parkinson_[~parkinson_.idx.str.contains(\"NLS_85\")]\n",
    "parkinson_ = parkinson_[~parkinson_.idx.str.contains(\"NLS_57\")]\n",
    "parkinson_ = parkinson_[~parkinson_.idx.str.contains(\"NLS_34\")]\n",
    "parkinson_ = parkinson_[~parkinson_.idx.str.contains(\"NLS_33\")]\n",
    "parkinson_ = parkinson_[~parkinson_.idx.str.contains(\"NLS_12\")]\n",
    "parkinson_ = parkinson_[~parkinson_.idx.str.contains(\"NLS_21\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "298ca6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = df.columns[3:].values.tolist()\n",
    "\n",
    "alzheimer_all_feat = np.array([alzheimer[feat] for feat in np.array(df.columns[3:])])\n",
    "others_all_feat = np.array([others[feat] for feat in np.array(df.columns[3:])])\n",
    "parkinson_all_feat = np.array([parkinson_[feat] for feat in np.array(df.columns[3:])])\n",
    "control_all_feat =np.array([control[feat] for feat in np.array(df.columns[3:])])\n",
    "#ataxia_all_feat =np.array([ataxia[feat] for feat in np.array(df.columns[3:])])\n",
    "\n",
    "\n",
    "with open('/export/b14/afavaro/SLT_submission/cognitive/results/cn_vs_pd.txt', 'w') as f:\n",
    " \n",
    "    kruskal(f, task, control_all_feat, parkinson_all_feat, \"controls vs.\", \"parkinson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "941137f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/export/b14/afavaro/SLT_submission/cognitive/results/cn_vs_otr.txt', 'w') as f:\n",
    "    \n",
    "    \n",
    "    kruskal(f, task, control_all_feat, others_all_feat, \"control vs.\", \"others\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7a7550ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/export/b14/afavaro/SLT_submission/cognitive/results/pd_vs_otr.txt', 'w') as f:\n",
    "    \n",
    "    \n",
    "    kruskal(f, task, parkinson_all_feat, others_all_feat, \"parkinson vs.\", \"others\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d6f0e515",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/export/b14/afavaro/SLT_submission/cognitive/results/ad_vs_otr.txt', 'w') as f:\n",
    "\n",
    "    \n",
    "    kruskal(f, task, alzheimer_all_feat, others_all_feat, \"alzheimers vs.\", \"others\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "396c8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/export/b14/afavaro/SLT_submission/cognitive/results/ad_vs_cn.txt', 'w') as f:\n",
    "    #final_sub\n",
    "    \n",
    "    kruskal(f, task, alzheimer_all_feat, control_all_feat, \"alzheimers vs.\", \"controls\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "195f4b47",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('/export/b14/afavaro/SLT_submission/cognitive/results/ad_vs_pd.txt', 'w') as f:\n",
    "    #final_sub\n",
    "    \n",
    "    kruskal(f, task, alzheimer_all_feat, parkinson_all_feat, \"alzheimers vs.\", \"parkinson\")\n",
    "\n",
    "\n",
    "kruskal_1 = read_stats_test('/export/b14/afavaro/SLT_submission/cognitive/results/ad_vs_cn.txt')\n",
    "\n",
    "kruskal_1_ = read_stats_test('/export/b14/afavaro/SLT_submission/cognitive/results/ry_ad_vs_cn.txt')\n",
    "kruskal_1__= read_stats_test('/export/b14/afavaro/SLT_submission/cognitive/results/rt_ad_vs_cn.txt')\n",
    "kruskal_2 = read_stats_test('/export/b14/afavaro/SLT_submission/cognitive/results/ad_vs_otr.txt')\n",
    "kruskal_3 = read_stats_test('/export/b14/afavaro/SLT_submission/cognitive/results/ad_vs_pd.txt')\n",
    "kruskal_4 = read_stats_test('/export/b14/afavaro/SLT_submission/cognitive/results/cn_vs_otr.txt')\n",
    "kruskal_4_ = read_stats_test('/export/b14/afavaro/SLT_submission/cognitive/results/ry_cn_vs_otr.txt')\n",
    "kruskal_4__ = read_stats_test('/export/b14/afavaro/SLT_submission/cognitive/results/ry_cn_vs_otr.txt')\n",
    "kruskal_4__ = read_stats_test('/export/b14/afavaro/SLT_submission/cognitive/results/rt_cn_vs_otr.txt')\n",
    "kruskal_5 = read_stats_test('/export/b14/afavaro/SLT_submission/cognitive/results/cn_vs_pd.txt')\n",
    "kruskal_5_ = read_stats_test('/export/b14/afavaro/SLT_submission/cognitive/results/rt_cn_vs_pd.txt')\n",
    "kruskal_6 = read_stats_test('/export/b14/afavaro/SLT_submission/cognitive/results/pd_vs_otr.txt')\n",
    "#best_krusk = compute_best_scores(kruskal)\n",
    "\n",
    "\n",
    "#PLOT DISTRIBUTIONS\n",
    "\n",
    "for i, title in enumerate(task):  \n",
    "    \n",
    "    nome = title\n",
    "   \n",
    "    controls = []\n",
    "    parkinson=[]\n",
    "    alz = []\n",
    "    #ataxia = []\n",
    "    others= []\n",
    "    \n",
    "    data = np.concatenate([control_all_feat[i], parkinson_all_feat[i], alzheimer_all_feat[i], others_all_feat[i]])\n",
    "    data = data.tolist()\n",
    "\n",
    "    [controls.append('CN') for value in range(len(control_all_feat[i]))]\n",
    "    #[ataxia.append('ATX') for value in range(len(ataxia_all_feat[i]))]\n",
    "    [parkinson.append('PD') for value in range(len(parkinson_all_feat[i]))]\n",
    "    [alz.append('AD') for value in range(len(alzheimer_all_feat[i]))]\n",
    "    [others.append('PDM') for value in range(len(others_all_feat[i]))]\n",
    "\n",
    "    lista = controls+parkinson+alz+others\n",
    "    \n",
    "    dict = {nome: data, 'Group': lista} \n",
    "    df = pd.DataFrame(dict)\n",
    "\n",
    "    sns.set(font_scale=2)\n",
    "    sns.catplot(x=nome, y=\"Group\", kind=\"boxen\", data=df, k_depth='full', palette=\"Blues\",\\\n",
    "               height=4.2, aspect=1.4, showfliers = False)\n",
    "    \n",
    "    plt.savefig(f'/export/b14/afavaro/SLT_submission/Final_plot_SLT/{nome}_', facecolor='white', dpi=600)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}