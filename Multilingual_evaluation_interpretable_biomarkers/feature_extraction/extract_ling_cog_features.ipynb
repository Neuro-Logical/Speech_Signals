{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## LINGUISTIC AND COGNITIVE FEATURES EXTRACTION\n",
    "\n",
    "This notebook shows how to apply the functions to extract linguistic and cognitive features from the speech transcripts contained in a csv file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lexicalrichness import LexicalRichness\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df: path containing speech transcripts stored in csv format. The csv should contain 3 main columns:\n",
    "\n",
    "#  1 - names: speaker ID for each subject recorded.\n",
    "#  2 - sentence transcripts\n",
    "#  3 - label representing the class (i.e., control (CN), Alzheimer's Disease (AD), Parkinson's Disease (PD)) if you want to conduct further analysis later.\n",
    "\n",
    "df = pd.read_csv(\"/export/b14/afavaro/LexicalRichness/transcripts_data.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%m\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#load the Spacy model for extracting data for English: \"en_core_web_sm\" \n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['sentence'] = df['sentences'].str.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a function to preprocess the text\n",
    "# List of english stopwords\n",
    "stopwords = list(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "  # Create Doc object\n",
    "    doc = nlp(text, disable=['ner'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "    return ' '.join(a_lemmas)\n",
    "\n",
    "df['Item'] = df['sentence'].apply(preprocess)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def count_words(string):\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "#Application to the raw data to get the full word count\n",
    "df['Word_Count'] = df['sentence'].apply(count_words)\n",
    "\n",
    "#Application to the preprocessed data to get the content-word count\n",
    "df['Word_Count_No_stop_words'] = df['Item'].apply(count_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def word_length(string):\n",
    "    #Get the length of the full text in characters\n",
    "    chars = len(string)\n",
    "    #Split the string into words\n",
    "    words = string.split()\n",
    "    #Compute the average word length and round the output to the second decimal point\n",
    "    if len(words)!=0:\n",
    "        avg_word_length = chars/len(words)\n",
    "   \n",
    "        return round(avg_word_length, 2)\n",
    "\n",
    "df['Avg_Word_Length'] = df['Item'].apply(word_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sentence_counter(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    #Initialize a counter variable\n",
    "    counter = 0\n",
    "    #Update the counter for each sentence which can be found in the doc.sents object returned by the Spacy model\n",
    "    for sentence in doc.sents:\n",
    "        counter = counter + 1\n",
    "    return counter\n",
    "#Note that this function is applied to the raw text in order to identify sentence boundaries\n",
    "df['Sentence_Count'] = df['sentence'].apply(sentence_counter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def avg_sent_length(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    #Initialize a counter variable\n",
    "    sent_number = 0\n",
    "    #Update the counter for each sentence which can be found in the doc.sents object returned by the Spacy model\n",
    "    for sent in doc.sents:\n",
    "        sent_number = sent_number + 1\n",
    "    #Get the number of words\n",
    "    words = text.split()\n",
    "    #Compute the average sentence length and round it to the second decimal point\n",
    "    avg_sent_length = len(words)/sent_number\n",
    "\n",
    "    return round(avg_sent_length, 2)\n",
    "\n",
    "#Note that this function is applied to the raw text in order to identify sentence boundaries\n",
    "df['Avg_Sent_Length_in_Words'] = df['sentence'].apply(avg_sent_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def nouns(text, model=nlp):\n",
    "\n",
    "    # Create doc object \n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "df['Noun_Count'] = df['Item'].apply(nouns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def verbs(text, model=nlp):\n",
    "    '''This function returns the number of verbs in an item'''\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of verbs\n",
    "    return pos.count('VERB')\n",
    "\n",
    "df['Verb_Count'] = df['Item'].apply(verbs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adjectives(text, model=nlp):\n",
    "\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adjectives\n",
    "    return pos.count('ADJ')\n",
    "\n",
    "df['Adjective_Count'] = df['Item'].apply(adjectives)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adverbs(text, model=nlp):\n",
    "\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adverbs\n",
    "    return pos.count('ADV')\n",
    "\n",
    "df['Adverb_Count'] = df['Item'].apply(adverbs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def numeral(text, model=nlp):\n",
    "\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adverbs\n",
    "    return pos.count('NUM')\n",
    "\n",
    "df['Numeral_Count'] = df['sentence'].apply(numeral) #meglio estrarlo dall'originale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def aux(text, model=nlp):\n",
    "\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adverbs\n",
    "    return pos.count('AUX')\n",
    "\n",
    "df['Auxiliary_Count'] = df['sentence'].apply(aux) #meglio estrarlo dall'originale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_nps(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    NP_count = 0\n",
    "    for np in doc.noun_chunks:\n",
    "        NP_count = NP_count + 1\n",
    "    return NP_count\n",
    "    #print(np)\n",
    "\n",
    "df['Number_of_NPs'] = df['Item'].apply(get_nps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_pps(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    pps = 0\n",
    "    for token in doc:\n",
    "        # You can try this with other parts of speech for different subtrees.\n",
    "        if token.pos_ == 'ADP':\n",
    "            \n",
    "            #Use the command below if you wanted to get the actual PPs\n",
    "            #pp = ' '.join([tok.orth_ for tok in token.subtree])\n",
    "            #This command counts the number of PPs\n",
    "            pps = pps + 1\n",
    "            \n",
    "    return pps\n",
    "\n",
    "df['Number_of_PPs'] = df['Item'].apply(get_pps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pattern = [{'POS': 'VERB', 'OP': '?'},\n",
    "           {'POS': 'ADV', 'OP': '*'},\n",
    "           {'POS': 'AUX', 'OP': '*'},\n",
    "           {'POS': 'VERB', 'OP': '+'}]\n",
    "\n",
    "\n",
    "def get_vps(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    vps = 0\n",
    "    # instantiate a Matcher instance\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Verb phrase\", [pattern], on_match=None) #new syntax of the command\n",
    "    # call the matcher to find matches \n",
    "    matches = matcher(doc)\n",
    "    spans = [doc[start:end] for _, start, end in matches]\n",
    "    for match in matches:\n",
    "        vps = vps +1\n",
    "    return vps\n",
    "    \n",
    "df['Number_of_VPs'] = df['Item'].apply(get_vps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def informational_verb(text):\n",
    "\n",
    "    cont_con = 0\n",
    "\n",
    "    if \"washing\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"overflowing\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"hanging\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"trying to help\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"falling\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"wobbling\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"drying\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"ignoring\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"reaching\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"reaching up\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"asking for cookie\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"laughing\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"standing\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "\n",
    "    return cont_con\n",
    "\n",
    "df['informational_verb'] = df['sentence'].apply(informational_verb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}