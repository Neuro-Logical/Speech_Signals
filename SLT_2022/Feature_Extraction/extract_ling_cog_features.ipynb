{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## LINGUISTIC AND COGNITIVE FEATURES EXTRACTION\n",
    "\n",
    "This notebook shows how to apply the functions to extract linguistic and cognitive features from the speech transcripts contained in a csv file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lexicalrichness import LexicalRichness\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df: path containing speech transcripts stored in csv format. The csv should contain 3 main columns:\n",
    "\n",
    "#  1 - names: speaker ID for each subject recorded.\n",
    "#  2 - sentence transcripts\n",
    "#  3 - label representing the class (i.e., control (CN), Alzheimer's Disease (AD), Parkinson's Disease (PD)) if you want to conduct further analysis later.\n",
    "\n",
    "df = pd.read_csv(\"/export/b14/afavaro/LexicalRichness/transcripts_data.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%m\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_lexical_diversity(transcript):\n",
    "    \n",
    "    lex = LexicalRichness(transcript)\n",
    "   # word_count = lex.words\n",
    "    unique_word_count =  lex.terms\n",
    "    type_token_ratio = lex.ttr\n",
    "   # root_type_token_ratio = lex.rttr\n",
    "    corrected_type_token_ratio = lex.cttr\n",
    "   # mean_segmental_type_token_ratio = lex.msttr(segment_window=12) #25\n",
    "    moving_average_type_token_ratio = lex.mattr(window_size=13) #25\n",
    "   # measure_textual_lexical_diversity= lex.mtld(threshold=0.72)\n",
    "   # hypergeometric_distribution_diversity = lex.hdd(draws=13)\n",
    "   # herdan_lexical_diversity_measure = lex.Herdan\n",
    "    summer_lexical_diversity_measure=lex.Summer\n",
    "    dugast_lexical_diversity_measure =lex.Dugast\n",
    "   # maas_lexical_diversity_measure = lex.Maas\n",
    "    \n",
    "    return unique_word_count, type_token_ratio, corrected_type_token_ratio, moving_average_type_token_ratio, summer_lexical_diversity_measure, dugast_lexical_diversity_measure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_files(data):\n",
    "    \n",
    "    speakers = data['idx'].tolist()\n",
    "    sentences = data['names'].tolist()\n",
    "    labels = data['label'].tolist()\n",
    "    lex_vals = np.array([compute_lexical_diversity(sent) for sent in sentences])\n",
    "    names = [\"unique_word_count\", \"type_token_ratio\", \"corrected_type_token_ratio\", \"moving_average_type_token_ratio\", \"summer_lexical_diversity_measure\", \"dugast_lexical_diversity_measure\"]\n",
    "    frame = pd.DataFrame({\"speakers\": speakers, \"labels\": labels, \"sentences\": sentences, **{name:val for name, val in zip(names,lex_vals.T)}})\n",
    "\n",
    "    return frame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = load_files(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#load the Spacy model for extracting data for English: \"en_core_web_sm\" \n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['sentence'] = df['sentences'].str.lower()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a function to preprocess the text\n",
    "#Customized list of stopwords\n",
    "stopwords = list(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "  # Create Doc object\n",
    "    doc = nlp(text, disable=['ner'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "    return ' '.join(a_lemmas)\n",
    "\n",
    "df['Item'] = df['sentence'].apply(preprocess)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def count_words(string):\n",
    "    # Split the string into words\n",
    "    words = string.split()\n",
    "    # Return the number of words\n",
    "    return len(words)\n",
    "\n",
    "#Application to the raw data to get the full word count\n",
    "df['Word_Count'] = df['sentence'].apply(count_words)\n",
    "\n",
    "#Application to the preprocessed data to get the content-word count\n",
    "df['Word_Count_No_stop_words'] = df['Item'].apply(count_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def word_length(string):\n",
    "    #Get the length of the full text in characters\n",
    "    chars = len(string)\n",
    "    #Split the string into words\n",
    "    words = string.split()\n",
    "    #Compute the average word length and round the output to the second decimal point\n",
    "    if len(words)!=0:\n",
    "        avg_word_length = chars/len(words)\n",
    "   \n",
    "        return round(avg_word_length, 2)\n",
    "\n",
    "df['Avg_Word_Length'] = df['Item'].apply(word_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def sentence_counter(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    #Initialize a counter variable\n",
    "    counter = 0\n",
    "    #Update the counter for each sentence which can be found in the doc.sents object returned by the Spacy model\n",
    "    for sentence in doc.sents:\n",
    "        counter = counter + 1\n",
    "    return counter\n",
    "#Note that this function is applied to the raw text in order to identify sentence boundaries\n",
    "df['Sentence_Count'] = df['sentence'].apply(sentence_counter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def avg_sent_length(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    #Initialize a counter variable\n",
    "    sent_number = 0\n",
    "    #Update the counter for each sentence which can be found in the doc.sents object returned by the Spacy model\n",
    "    for sent in doc.sents:\n",
    "        sent_number = sent_number + 1\n",
    "    #Get the number of words\n",
    "    words = text.split()\n",
    "    #Compute the average sentence length and round it to the second decimal point\n",
    "    avg_sent_length = len(words)/sent_number\n",
    "\n",
    "    return round(avg_sent_length, 2)\n",
    "\n",
    "#Note that this function is applied to the raw text in order to identify sentence boundaries\n",
    "df['Avg_Sent_Length_in_Words'] = df['sentence'].apply(avg_sent_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def nouns(text, model=nlp):\n",
    "\n",
    "    # Create doc object \n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "df['Noun_Count'] = df['Item'].apply(nouns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def verbs(text, model=nlp):\n",
    "    '''This function returns the number of verbs in an item'''\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of verbs\n",
    "    return pos.count('VERB')\n",
    "\n",
    "df['Verb_Count'] = df['Item'].apply(verbs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adjectives(text, model=nlp):\n",
    "\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adjectives\n",
    "    return pos.count('ADJ')\n",
    "\n",
    "df['Adjective_Count'] = df['Item'].apply(adjectives)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adverbs(text, model=nlp):\n",
    "\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adverbs\n",
    "    return pos.count('ADV')\n",
    "\n",
    "df['Adverb_Count'] = df['Item'].apply(adverbs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def numeral(text, model=nlp):\n",
    "\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adverbs\n",
    "    return pos.count('NUM')\n",
    "\n",
    "df['Numeral_Count'] = df['sentence'].apply(numeral) #meglio estrarlo dall'originale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def aux(text, model=nlp):\n",
    "\n",
    "    # Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    # Return number of adverbs\n",
    "    return pos.count('AUX')\n",
    "\n",
    "df['Auxiliary_Count'] = df['sentence'].apply(aux) #meglio estrarlo dall'originale"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_nps(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    NP_count = 0\n",
    "    for np in doc.noun_chunks:\n",
    "        NP_count = NP_count + 1\n",
    "    return NP_count\n",
    "    #print(np)\n",
    "\n",
    "df['Number_of_NPs'] = df['Item'].apply(get_nps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_pps(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    pps = 0\n",
    "    for token in doc:\n",
    "        # You can try this with other parts of speech for different subtrees.\n",
    "        if token.pos_ == 'ADP':\n",
    "            \n",
    "            #Use the command below if you wanted to get the actual PPs\n",
    "            #pp = ' '.join([tok.orth_ for tok in token.subtree])\n",
    "            #This command counts the number of PPs\n",
    "            pps = pps + 1\n",
    "            \n",
    "    return pps\n",
    "\n",
    "df['Number_of_PPs'] = df['Item'].apply(get_pps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pattern = [{'POS': 'VERB', 'OP': '?'},\n",
    "           {'POS': 'ADV', 'OP': '*'},\n",
    "           {'POS': 'AUX', 'OP': '*'},\n",
    "           {'POS': 'VERB', 'OP': '+'}]\n",
    "\n",
    "\n",
    "def get_vps(text):\n",
    "\n",
    "    doc = nlp(text)\n",
    "    vps = 0\n",
    "    # instantiate a Matcher instance\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    matcher.add(\"Verb phrase\", [pattern], on_match=None) #new syntax of the command\n",
    "    # call the matcher to find matches \n",
    "    matches = matcher(doc)\n",
    "    spans = [doc[start:end] for _, start, end in matches]\n",
    "    for match in matches:\n",
    "        vps = vps +1\n",
    "    return vps\n",
    "    \n",
    "df['Number_of_VPs'] = df['Item'].apply(get_vps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Connectives to instruct, recount and sequence\n",
    "temporal_connectives = ['afterwards', 'once', 'at this moment', 'at this point', 'before', 'finally', \n",
    "                        'here', 'in the end', 'lastly', 'later on', 'meanwhile', 'next', 'now', \n",
    "                        'on another occasion', 'previously','since', 'soon', 'straightaway', 'then', \n",
    "                        'when', 'whenever', 'while']\n",
    "\n",
    "#Connectives to show cause or conditions\n",
    "causal_connectives = ['accordingly', 'all the same', 'an effect of', 'an outcome of', 'an upshot of',\n",
    "                      'as a consequence of', 'as a result of', 'because', 'caused by', 'consequently',\n",
    "                      'despite this', 'even though', 'hence', 'however', 'in that case', 'moreover',\n",
    "                      'nevertheless', 'otherwise', 'so', 'so as', 'stemmed from', 'still', 'then',\n",
    "                      'therefore', 'though', 'under the circumstances', 'yet']\n",
    "\n",
    "#Connectives for showing results\n",
    "exemplifying_connectives = ['accordingly', 'as a result', 'as exemplified by', 'consequently', 'for example',\n",
    "                            'for instance', 'for one thing', 'including', 'provided that', 'since', 'so',\n",
    "                            'such as', 'then', 'therefore', 'these include', 'through', 'unless', 'without']\n",
    "\n",
    "#Connectives to show similarity or add a point\n",
    "additive_connectives = ['and', 'additionally', 'also', 'as well', 'even', 'furthermore', 'in addition', 'indeed',\n",
    "                        'let alone', 'moreover', 'not only']\n",
    "\n",
    "#Connectives showing a difference or an opposite point of view\n",
    "contrastive_connectives = ['alternatively', 'anyway', 'but', 'by contrast', 'differs from', 'elsewhere',\n",
    "                           'even so', 'however', 'in contrast', 'in fact', 'in other respects', 'in spite of this',\n",
    "                           'in that respect', 'instead', 'nevertheless', 'on the contrary', 'on the other hand',\n",
    "                           'rather', 'though', 'whereas', 'yet']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def temporal_connectives_count(text):\n",
    "\n",
    "    count = 0\n",
    "    for string in temporal_connectives:\n",
    "        for match in re.finditer(string, text):\n",
    "            count +=  1\n",
    "\n",
    "    return count\n",
    "\n",
    "#Note that we apply the function to the raw text (and remember that it is important to lowercase all words)\n",
    "df['Temporal_Connectives_Count'] = df['sentence'].apply(temporal_connectives_count)\n",
    "#df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def causal_connectives_count(text):\n",
    "\n",
    "    count = 0\n",
    "    for string in causal_connectives:\n",
    "        for match in re.finditer(string, text):\n",
    "            count +=  1\n",
    "    return count\n",
    "\n",
    "df['Causal_Connectives_Count'] = df['sentence'].apply(causal_connectives_count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def exemplifying_connectives_count(text):\n",
    "\n",
    "    count = 0\n",
    "    for string in exemplifying_connectives:\n",
    "        for match in re.finditer(string, text):\n",
    "            count +=  1\n",
    "    return count\n",
    "\n",
    "df['Exemplifying_Connectives_Count'] = df['sentence'].apply(exemplifying_connectives_count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def additive_connectives_count(text):\n",
    "\n",
    "    count = 0\n",
    "    for string in additive_connectives:\n",
    "        for match in re.finditer(string, text):\n",
    "            count +=  1\n",
    "    return count\n",
    "\n",
    "df['Additive_Connectives_Count'] = df['sentence'].apply(additive_connectives_count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def contrastive_connectives_count(text):\n",
    "\n",
    "    cont_con = 0\n",
    "    for string in contrastive_connectives:\n",
    "        if string in text:\n",
    "            cont_con = cont_con + 1\n",
    "    return cont_con\n",
    "\n",
    "df['Contrastive_Connectives_Count'] = df['sentence'].apply(contrastive_connectives_count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filled_pause = [\"uhm\"]\n",
    "\n",
    "def filled_pauses(text):\n",
    "    \n",
    "    cont_pauses = 0\n",
    "    for string in filled_pause:\n",
    "        for match in re.finditer(string, text):\n",
    "            cont_pauses += 1\n",
    "    return cont_pauses\n",
    "\n",
    "df['Filled_Pauses'] = df['sentence'].apply(filled_pauses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def uncertainty(text):\n",
    "\n",
    "    cont_con = 0\n",
    "    if \"?\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"why\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"might\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"can\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"may\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"sure\" in text:\n",
    "        cont_con = cont_con + 1     \n",
    "   # if \"I\" in text:\n",
    "      #  cont_con = cont_con + 1 \n",
    "    if \"uhm\" in text:\n",
    "        cont_con = cont_con + 1 \n",
    "    if \"ah\" in text:\n",
    "        cont_con = cont_con + 1 \n",
    "    if \"should\" in text:\n",
    "        cont_con = cont_con + 1 \n",
    "    if \"looks like\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "\n",
    "    return cont_con\n",
    "\n",
    "df['uncertainty'] = df['sentence'].apply(certanty)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def repetitions(text):\n",
    "\n",
    "    repetition = 0\n",
    "    text = text.split()\n",
    "    d = dict()\n",
    "    \n",
    "    for line in text:\n",
    "        line = line.strip()\n",
    "        line = line.lower()\n",
    "        words = line.split(\" \")\n",
    "        for word in words:\n",
    "\n",
    "            if word in d:\n",
    "                d[word] = d[word] + 1\n",
    "            else:\n",
    "                d[word] = 1\n",
    "    \n",
    "    for key in list(d.keys()):\n",
    "        if key not in stopwords:\n",
    "            if d[key] > 1:\n",
    "                repetition +=1\n",
    "\n",
    "    return repetition\n",
    "\n",
    "df['repetition'] = df['sentence'].apply(repetitions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def informational_verb(text):\n",
    "\n",
    "    cont_con = 0\n",
    "\n",
    "    if \"washing\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"overflowing\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"hanging\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"trying to help\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"falling\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"wobbling\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"drying\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"ignoring\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"reaching\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"reaching up\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"asking for cookie\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"laughing\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"standing\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "\n",
    "    return cont_con\n",
    "\n",
    "df['informational_verb'] = df['sentence'].apply(informational_verb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def informational_content(text):\n",
    "    \n",
    "    cont_con = 0\n",
    "    \n",
    "    if \"mother\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"sister\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"cookie\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"cookie jar\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"curtains\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"cabinet\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"brother\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"kitchen\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"sink\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"garden\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"fall\" in text:\n",
    "        cont_con = cont_con + 1\n",
    "    if \"dishes\" in text:\n",
    "        cont_con = cont_con + 1     \n",
    "    if \"stool\" in text:\n",
    "        cont_con = cont_con + 1 \n",
    "    if \"poddle\" in text:\n",
    "        cont_con = cont_con + 1 \n",
    "    \n",
    "    return cont_con\n",
    "\n",
    "df['informational'] = df['sentence'].apply(informational_content)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}